{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0e5c1-6456-4c10-9307-e58b3fd88656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-23 18:20:10 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230223-182010.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-23 18:20:15 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "from graphnet.data.sqlite.sqlite_utilities import create_table\n",
    "\n",
    "def load_input(meta_batch: pd.DataFrame, geometry_table : pd.DataFrame, input_data_folder: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Will load the corresponding detector readings associated with the meta data batch.\n",
    "        \"\"\"\n",
    "        batch_id = pd.unique(meta_batch['batch_id'])\n",
    "\n",
    "        assert len(batch_id) == 1, \"contains multiple batch_ids. Did you set the batch_size correctly?\"\n",
    "\n",
    "        detector_readings = pd.read_parquet(path = f'{input_data_folder}/batch_{batch_id[0]}.parquet')\n",
    "        sensor_positions = geometry_table.loc[detector_readings['sensor_id'], ['x', 'y', 'z']]\n",
    "        sensor_positions.index = detector_readings.index\n",
    "\n",
    "        for column in sensor_positions.columns:\n",
    "            if column not in detector_readings.columns:\n",
    "                detector_readings[column] = sensor_positions[column]\n",
    "\n",
    "        detector_readings['auxiliary'] = detector_readings['auxiliary'].replace({True: 1, False: 0})\n",
    "        return detector_readings.reset_index()\n",
    "\n",
    "def add_to_table(database_path: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      table_name:  str,\n",
    "                      is_primary_key: bool,\n",
    "                      ) -> None:\n",
    "    \"\"\"Writes meta data to sqlite table. \n",
    "\n",
    "    Args:\n",
    "        database_path (str): the path to the database file.\n",
    "        df (pd.DataFrame): the dataframe that is being written to table.\n",
    "        table_name (str, optional): The name of the meta table. Defaults to 'meta_table'.\n",
    "        is_primary_key(bool): Must be True if each row of df corresponds to a unique event_id. Defaults to False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(database_path)\n",
    "        create_table(   columns=  df.columns,\n",
    "                        database_path = database_path, \n",
    "                        table_name = table_name,\n",
    "                        integer_primary_key= is_primary_key,\n",
    "                        index_column = 'event_id')\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if 'already exists' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise e\n",
    "    engine = sqlalchemy.create_engine(\"sqlite:///\" + database_path)\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists=\"append\", chunksize = 200000)\n",
    "    engine.dispose()\n",
    "    return\n",
    "\n",
    "def convert_to_sqlite(meta_data_path: str,\n",
    "                      database_path: str,\n",
    "                      input_data_folder: str,\n",
    "                      geometry_table : pd.DataFrame,\n",
    "                      batch_size: int = 200000,\n",
    "                      accepted_batch_ids=None) -> None:\n",
    "    \"\"\"Converts a selection of the Competition's parquet files to a single sqlite database.\n",
    "\n",
    "    Args:\n",
    "        meta_data_path (str): Path to the meta data file.\n",
    "        batch_size (int): the number of rows extracted from meta data file at a time. Keep low for memory efficiency.\n",
    "        database_path (str): path to database. E.g. '/my_folder/data/my_new_database.db'\n",
    "        input_data_folder (str): folder containing the parquet input files.\n",
    "        accepted_batch_ids (List[int]): The batch_ids you want converted. Defaults to None (all batches will be converted)\n",
    "    \"\"\"\n",
    "    meta_data_iter = pq.ParquetFile(meta_data_path).iter_batches(batch_size = batch_size)\n",
    "    \n",
    "    if not database_path.endswith('.db'):\n",
    "        database_path = database_path+'.db'\n",
    "        \n",
    "    converted_batches = [] \n",
    "    progress_bar = tqdm(total = None)\n",
    "    for meta_data_batch in meta_data_iter:\n",
    "        unique_batch_ids = pd.unique(meta_data_batch['event_id']).tolist()\n",
    "        meta_data_batch  = meta_data_batch.to_pandas()\n",
    "        if accepted_batch_ids is not None:\n",
    "            meta_data_batch = meta_data_batch.loc[meta_data_batch['batch_id'].isin(accepted_batch_ids)].copy()\n",
    "        if len(meta_data_batch) == 0:\n",
    "            continue\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = meta_data_batch,\n",
    "                    table_name='meta_table',\n",
    "                    is_primary_key= True)\n",
    "        pulses = load_input(meta_batch=meta_data_batch, geometry_table=geometry_table, input_data_folder= input_data_folder)\n",
    "        del meta_data_batch # memory\n",
    "        add_to_table(database_path = database_path,\n",
    "                    df = pulses,\n",
    "                    table_name='pulse_table',\n",
    "                    is_primary_key= False)\n",
    "        del pulses # memory\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    del meta_data_iter # memory\n",
    "    print(f'Conversion Complete!. Database available at\\n {database_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560c2f68-30c5-4554-a2bf-88b600a94936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../data/test_database.db': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/test_database.db\n",
      "../data/test_database.db\n",
      "Conversion Complete!. Database available at\n",
      " ../data/test_database.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!rm '../data/test_database.db'\n",
    "input_data_folder = '../data/test'\n",
    "geometry_table = pd.read_csv('../data/sensor_geometry.csv')\n",
    "meta_data_path = '../data/test_meta.parquet'\n",
    "\n",
    "database_path = '../data/test_database'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder,\n",
    "                  geometry_table=geometry_table,\n",
    "                  accepted_batch_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e25ef3-ff9f-4598-893e-bf34749c4610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_database_52_53.db\n",
      "../data/train_database_52_53.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [13:27, 807.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_database_52_53.db\n",
      "../data/train_database_52_53.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [32:51, 985.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Complete!. Database available at\n",
      " ../data/train_database_52_53.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!rm '../data/train_database.db'\n",
    "input_data_folder = '../data/train'\n",
    "geometry_table = pd.read_csv('../data/sensor_geometry.csv')\n",
    "meta_data_path = '../data/train_meta.parquet'\n",
    "\n",
    "database_path = '../data/train_database_52_53'\n",
    "convert_to_sqlite(meta_data_path,\n",
    "                  database_path=database_path,\n",
    "                  input_data_folder=input_data_folder,\n",
    "                  geometry_table=geometry_table,\n",
    "                  accepted_batch_ids=[52, 53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d8eb2b-ce89-407b-9ada-5d0731c41987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.adam import Adam\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, ZenithReconstructionWithKappa, AzimuthReconstructionWithKappa\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from pytorch_lightning import Trainer\n",
    "import pandas as pd\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def build_model(config: Dict[str,Any], train_dataloader: Any) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == 'direction':\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "        additional_attributes = ['zenith', 'azimuth', 'event_id']\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,\n",
    "        optimizer_kwargs={\"lr\": 1e-03, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-02, 1, 1e-02],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_pretrained_model(config: Dict[str,Any], state_dict_path: str) -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config = config)\n",
    "    model = build_model(config = config, \n",
    "                        train_dataloader = train_dataloader)\n",
    "    #model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [config[\"target\"] + \"_x\", \n",
    "                              config[\"target\"] + \"_y\", \n",
    "                              config[\"target\"] + \"_z\", \n",
    "                              config[\"target\"] + \"_kappa\" ]\n",
    "    model.additional_attributes = ['event_id'] #'zenith', 'azimuth',  not available in test data\n",
    "    return model\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    \n",
    "    train_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['train_selection'])[config['index_column']].ravel().tolist() if config['train_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = True,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    validate_dataloader = make_dataloader(db = config['path'],\n",
    "                                            selection = pd.read_csv(config['validate_selection'])[config['index_column']].ravel().tolist() if config['validate_selection'] else None,\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = {'direction': Direction()},\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                          \n",
    "                                            )\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(db = config['inference_database_path'],\n",
    "                                            selection = None, # Entire database\n",
    "                                            pulsemaps = config['pulsemap'],\n",
    "                                            features = features,\n",
    "                                            truth = truth,\n",
    "                                            batch_size = config['batch_size'],\n",
    "                                            num_workers = config['num_workers'],\n",
    "                                            shuffle = False,\n",
    "                                            labels = None, # Cannot make labels in test data\n",
    "                                            index_column = config['index_column'],\n",
    "                                            truth_table = config['truth_table'],\n",
    "                                            )\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus = [0],\n",
    "        dataloader = test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=['event_id']\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aeea53-29c2-4268-8263-4a7cfefa15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df, angle_post_fix = '_reco', vec_post_fix = '') -> pd.DataFrame:\n",
    "    r = np.sqrt(df['direction_x'+ vec_post_fix]**2 + df['direction_y'+ vec_post_fix]**2 + df['direction_z' + vec_post_fix]**2)\n",
    "    df['zenith' + angle_post_fix] = np.arccos(df['direction_z'+ vec_post_fix]/r)\n",
    "    df['azimuth'+ angle_post_fix] = np.arctan2(df['direction_y'+ vec_post_fix],df['direction_x' + vec_post_fix]) #np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n",
    "    df['azimuth'+ angle_post_fix][df['azimuth'  + angle_post_fix]<0] = df['azimuth'  + angle_post_fix][df['azimuth'  +  angle_post_fix]<0] + 2*np.pi \n",
    "\n",
    "    drop_these_columns = []\n",
    "    for column in results.columns:\n",
    "        if column not in ['event_id', 'zenith', 'azimuth']:\n",
    "            drop_these_columns.append(column)\n",
    "    return df.drop(columns = drop_these_columns).iloc[:,[0,2,1]].set_index('event_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309dde75-c6a2-4a2d-87fc-fd28c3448d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-23 18:21:47 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-23 18:21:47 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-23 18:21:47 - SQLiteDataset.warning - Removing the following (missing) truth variables: zenith, azimuth\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1c28d3e7e146c38d7316e7093a8b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "features = FEATURES.KAGGLE\n",
    "truth = TRUTH.KAGGLE\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "        \"path\": '../data/test_database.db',\n",
    "        \"inference_database_path\": '../data/test_database.db',\n",
    "        \"pulsemap\": 'pulse_table',\n",
    "        \"truth_table\": 'meta_table',\n",
    "        \"features\": features,\n",
    "        \"truth\": truth,\n",
    "        \"index_column\": 'event_id',\n",
    "        \"run_name_tag\": 'graphnet_baseline_submission_test',\n",
    "        \"batch_size\": 200,\n",
    "        \"num_workers\": 4,\n",
    "        \"target\": 'direction',\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \"fit\": {\n",
    "                \"max_epochs\": 50,\n",
    "                \"gpus\": [0],\n",
    "                \"distribution_strategy\": None,\n",
    "                },\n",
    "        'train_selection': None,\n",
    "        'validate_selection':  None,\n",
    "        'test_selection': None,\n",
    "        'base_dir': 'training'\n",
    "}\n",
    "model   = load_pretrained_model(config = config, \n",
    "                                state_dict_path='../dynedge_pretrained_batch_1_to_50/state_dict.pth')\n",
    "results = inference(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2c08fb-5010-40f0-984c-e62f18c7490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = prepare_dataframe(results, angle_post_fix = '')\n",
    "submission_df.to_csv('../submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a285c5-876f-4cc0-914e-09c155e54453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
